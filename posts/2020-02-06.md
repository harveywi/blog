# The Information Theory of Types

## Introduction
We begin this misadventure with [a game](https://en.wikipedia.org/wiki/The_Game_(mind_game)):

While Alice is hacking on the neatest piece of software of all time, she realizes that she needs to implement a function (let's call it `f`).  However, she's too busy to do it herself, so she askes her buddy Bob to implement it instead (yielding his implementation that we will call `g`).

If Bob does it correctly (and `f == g` for some definition of equality that we will explore later), Alice gets a million bucks (a typical bonus for a programmer) and Bob gets a dopamine hit (pretty rare for a programmer, don't waste them).

Our goal is to help Alice and Bob with the task.

I should also mention that they are in a hurry and can't afford to go get a couple PhDs or spend thirty years and the equivalent of a small country's GDP studying or brute-forcing the problem.  And they have average intelligence, stamina, and charisma.

A few questions arise:

1. How should Alice formulate the [specification](https://en.wikipedia.org/wiki/Formal_specification) of `f`?

2. How should Alice convey that specification to Bob?

3. How should Bob use the specification to ensure he's implementing `g` properly?

4. When Bob sends `g` back to Alice, how can she realistically check the two for equivalence?

Fortunately, we get to cheat and bend the rules of the game if we find that Alice and Bob cannot win.  However, our goal is to bend the rules as little as possible so that they can be proud and blog about it.

## Types and Type Inhabitants
[Type systems](https://en.wikipedia.org/wiki/Type_system) and [type theory](https://en.wikipedia.org/wiki/Type_theory) are super handy [lightweight formal methods](https://en.wikipedia.org/wiki/Formal_methods#Lightweight_formal_methods) that can help us reduce software defects by making [illegal states unrepresentable](https://twitter.com/yminsky/status/1034947939364425731), adding mathematical structure and semantics for implementing [code completion tooling](https://en.wikipedia.org/wiki/Intelligent_code_completion#IntelliSense), enabling massive refactoring without breaking complex software systems, and so on.  Types are nice.

Not all type systems are created equal.  The type systems that most of you are probably familiar with (e.g., Java, C#, C, etc.) tend to be somewhat ad hoc, relatively wimpy, and some are even unsound [[1](https://en.wikipedia.org/wiki/Soundness)] [[2](https://gist.github.com/paulp/4525943)].  The [lambda cube](https://en.wikipedia.org/wiki/Lambda_cube) shows a few dimensions of variability of type systems based on their power and expressivity, and gives an idea of the primary ways to make a given type system more powerful (e.g. adding polymorphism, dependent types, or type operators).  There are all sorts of neat type systems out there that do not seem to clearly (at least to me) squeeze into the lambda cube structure, such as systems with [refinement types](https://ucsd-progsys.github.io/liquidhaskell-blog/), [row polymorphism](https://en.wikipedia.org/wiki/Row_polymorphism), etc.  It's a big world!

Types carry profound meaning.  The [Curry-Howard correspondence](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence) tells us that types are isomorphic to logical propositions.  Values of a type (also called *type inhabitants*) are proofs of that type's corresponding logical proposition.  [Philip Wadler's Propositions as Types](https://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf) is a fascinating and fairly thorough exploration of this idea, as is the first chapter of [the HoTT book](https://homotopytypetheory.org/book/).

Alex Konovalov recently wrote a blog post, "[Counting type inhabitants](https://alexknvl.com/posts/counting-type-inhabitants.html)" that describes a method for counting the number of distinct values that inhabit a given type, where distinctness is defined according to [observational equivalence](https://en.wikipedia.org/wiki/Observational_equivalence).  There are other excellent resources on this topic, including Joel Burget's "[The algebra (and calculus!) of algebraic data types](https://codewords.recurse.com/issues/three/algebra-and-calculus-of-algebraic-data-types)".

## The Entropy of a Type
Given that we can count a type `T`'s inhabitants, it is natural and potentially 
useful to model `T` as a random variable.  In doing so, we can derive a measure of [*Shannon entropy*](https://en.wikipedia.org/wiki/Entropy_(information_theory)) `H(T)` of `T` via the usual formulation
`H(T) = -sum(P(x) * log(P(x)) for x in T)`.  This tells us that the task of singling out a particular inhabitant of `T` from all of `T`'s other inhabitants will require `log2(|T|)` bits of information (assuming all inhabitants are equally likely).  If the inhabitants of `T` were
to somehow have unequal probabilities, then `T` would become more predictable, and
would therefore have lower entropy.

There are several variants on the theme of Shannon entropy, including conditional and joint
entropy.  Given two random variables `X` and `Y`, the conditional entropy `H(Y|X)` answers the question, "Given that I know the value of `X`, how much information do I need to describe the outcome
of `Y`?"  Joint entropy `H(X, Y)` is the Shannon entropy that uses probabilities of
co-occurring pairs of values `(x, y)` chosen from `X` and `Y`, respectively.

Conditional entropy and joint entropy fold nicely into something called [mutual information](https://en.wikipedia.org/wiki/Mutual_information), denoted `I(X; Y)`.  There are several equivalent ways
to express it:

- `H(X) - H(X|Y)`
- `H(Y) - H(Y|X)`
- `H(X) + H(Y) - H(X, Y)`
- `H(X, Y) - H(X|Y) - H(Y|X)`

Interestingly, we can take mutual information and derive a [metric](https://en.wikipedia.org/wiki/Metric_(mathematics)) from it!  The additional mathematical structure provided by a metric
can be really useful for using entropy in machine learning, visualization, clustering,
etc.  The metric, which we will call `d(X, Y)`, can be written in all of the following
equivalent ways:

- `H(X, Y) - I(X;Y)`
- `H(X) + H(Y) - 2 * I(X; Y)`
- `H(X|Y) + H(Y|X)`

An observation that is interesting, and could be potentially useful:  The second formulation (`H(X) + H(Y) - 2 * I(X; Y)`) has a striking similarity to the squared Euclidean distance.  Given two vectors `v1` and `v2`, the squared Euclidean distance between 
them is:

```
|v1 - v2| = (v1 路 v1) + (v2 路 v2) - 2 * (v1 路 v2)
```

where `路` denotes the [dot product](https://en.wikipedia.org/wiki/Dot_product).  Note that we can rewrite the second formulation above solely
in terms of mutual information to make the similarity even more obvious:

```
I(X; Y) = I(X; X) + I(Y; Y) - 2 * I(X; Y)
```

So, for the purposes of creativity and intuition, you can think of `H(X)` as a sort of "squared Euclidean norm of X" or "dot product of X with itself", and `I(X; Y)` to be a sort of dot product
(or, more generally, [inner product](https://en.wikipedia.org/wiki/Inner_product_space)) of
`X` and `Y`.  (Note to reader:  Someone has probably already formalized this connection.  We may
revisit it in the near future if we find it useful!)

## Next Steps:
In the next post, we will see if we can apply these information theory
concepts to the rules of the game.  We may find that these principles help Alice
develop a tight specification for `f`, and also help her communicate the spec
to Bob.  We may also discover what Bob should provide back to Alice so that she
can verify that `g` is equivalent (up to observation) to `f`.

Stay tuned.
